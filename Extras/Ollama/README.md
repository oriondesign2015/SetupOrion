# Ollama + LiteLLM Stack

## üìã Vis√£o Geral

Este stack combina **Ollama** (runtime local para LLMs) com **LiteLLM** (gateway de API compat√≠vel com OpenAI) para fornecer uma solu√ß√£o completa de IA local com interface padronizada.

**‚ú® Instala√ß√£o Standalone**: Este stack pode ser instalado e usado **independentemente** de qualquer outro sistema. N√£o requer o SetupOrion ou qualquer outra ferramenta.

> ‚ö†Ô∏è **IMPORTANTE**: O SetupOrion possui sua pr√≥pria implementa√ß√£o integrada do Ollama + LiteLLM.
> Este diret√≥rio `Extras/Ollama` √© um **projeto standalone separado** que pode ser usado de forma independente.

### O que est√° inclu√≠do:

- **Ollama**: Execute modelos de linguagem localmente (Llama, Mistral, Phi, etc.)
- **LiteLLM**: Gateway de API compat√≠vel com OpenAI para Ollama
- **PostgreSQL**: Banco de dados para rastreamento de gastos e gerenciamento de chaves (opcional)

## üéØ Caracter√≠sticas

- ‚úÖ API compat√≠vel com OpenAI (use com qualquer SDK OpenAI)
- ‚úÖ Suporte para m√∫ltiplos modelos (Llama 3.2, Mistral, Phi-3, etc.)
- ‚úÖ Gerenciamento de chaves virtuais
- ‚úÖ Rastreamento de uso e gastos
- ‚úÖ Rate limiting e budgets
- ‚úÖ Interface web de administra√ß√£o
- ‚úÖ Suporte para GPU (NVIDIA)
- ‚úÖ Totalmente local e privado
- ‚úÖ **Instala√ß√£o standalone** - n√£o depende de outros sistemas

## üì¶ Pr√©-requisitos

### Requisitos M√≠nimos:
- **CPU**: 4 cores
- **RAM**: 8GB (16GB recomendado)
- **Disco**: 20GB de espa√ßo livre
- **Docker**: 20.10 ou superior
- **Docker Compose**: 2.0 ou superior

### Requisitos para GPU (Opcional):
- **GPU NVIDIA** com suporte CUDA
- **NVIDIA Container Toolkit** instalado

## üöÄ Instala√ß√£o

### M√©todo 1: Instala√ß√£o Autom√°tica (Recomendado)

```bash
# 1. Clone ou baixe os arquivos
git clone https://github.com/oriondesign2015/SetupOrion.git
cd SetupOrion/Extras/Ollama

# 2. Execute o instalador
bash install.sh
```

O instalador ir√°:
- ‚úÖ Verificar depend√™ncias (Docker, Docker Compose)
- ‚úÖ Criar diret√≥rio de instala√ß√£o em `~/ollama-litellm`
- ‚úÖ Copiar todos os arquivos necess√°rios
- ‚úÖ Gerar chaves de seguran√ßa automaticamente
- ‚úÖ Detectar PostgreSQL existente (se houver)
- ‚úÖ Configurar GPU NVIDIA (se dispon√≠vel)
- ‚úÖ Iniciar os servi√ßos
- ‚úÖ Opcionalmente baixar um modelo LLM

### M√©todo 2: Instala√ß√£o Manual

```bash
# 1. Crie um diret√≥rio para a instala√ß√£o
mkdir -p ~/ollama-litellm
cd ~/ollama-litellm

# 2. Baixe os arquivos necess√°rios
# Copie os seguintes arquivos para este diret√≥rio:
# - docker-compose.yml
# - docker-compose.no-postgres.yml
# - litellm_config.yaml
# - .env.example

# 3. Configure as vari√°veis de ambiente
cp .env.example .env
nano .env  # Edite e altere as senhas e chaves
```

**‚ö†Ô∏è IMPORTANTE**: Altere pelo menos estas vari√°veis:
- `LITELLM_MASTER_KEY`: Chave mestra para admin
- `LITELLM_SALT_KEY`: Chave para criptografia (N√ÉO MUDE ap√≥s primeira configura√ß√£o!)
- `POSTGRES_PASSWORD`: Senha do banco de dados

```bash
# 4. Inicie os servi√ßos
docker-compose up -d
```

### 4. Baixe um modelo no Ollama

```bash
# Baixar Llama 3.2 (3B - r√°pido e eficiente)
docker exec -it ollama ollama pull llama3.2

# Ou Llama 3.1 (8B - mais poderoso)
docker exec -it ollama ollama pull llama3.1

# Ou Mistral (7B - √≥timo equil√≠brio)
docker exec -it ollama ollama pull mistral
```

### 5. Teste a instala√ß√£o

```bash
curl http://localhost:4000/health
```

## üìñ Uso

### Acessar a Interface Web

Abra seu navegador em: `http://localhost:4000/ui`

Use a chave mestra (`LITELLM_MASTER_KEY`) para fazer login.

### Fazer uma requisi√ß√£o via API

```bash
curl -X POST 'http://localhost:4000/chat/completions' \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer sk-1234' \
  -d '{
    "model": "llama3.2",
    "messages": [
      {
        "role": "user",
        "content": "Ol√°! Como voc√™ est√°?"
      }
    ]
  }'
```

### Usar com Python (OpenAI SDK)

```python
from openai import OpenAI

client = OpenAI(
    api_key="sk-1234",  # Sua LITELLM_MASTER_KEY
    base_url="http://localhost:4000"
)

response = client.chat.completions.create(
    model="llama3.2",
    messages=[
        {"role": "user", "content": "Explique o que √© IA em 3 linhas"}
    ]
)

print(response.choices[0].message.content)
```

### Criar uma chave virtual

```bash
curl -X POST 'http://localhost:4000/key/generate' \
  -H 'Authorization: Bearer sk-1234' \
  -H 'Content-Type: application/json' \
  -d '{
    "key_alias": "minha-app",
    "rpm_limit": 10,
    "max_budget": 100
  }'
```

## üîß Configura√ß√£o Avan√ßada

### Usar PostgreSQL Externo

Se voc√™ j√° tem um PostgreSQL instalado, edite o `.env`:

```bash
USE_EXTERNAL_POSTGRES=true
EXTERNAL_POSTGRES_HOST=seu-host-postgres
EXTERNAL_POSTGRES_PORT=5432
EXTERNAL_POSTGRES_DB=litellm
EXTERNAL_POSTGRES_USER=litellm
EXTERNAL_POSTGRES_PASSWORD=sua-senha
```

Depois, comente a se√ß√£o `postgres` no `docker-compose.yml` e atualize a vari√°vel `DATABASE_URL` no servi√ßo `litellm`.

### Habilitar GPU (NVIDIA)

1. Instale o NVIDIA Container Toolkit
2. Descomente a se√ß√£o `deploy` no servi√ßo `ollama` do `docker-compose.yml`
3. Reinicie os containers

```bash
docker-compose down
docker-compose up -d
```

### Adicionar mais modelos

Edite `litellm_config.yaml` e adicione novos modelos na se√ß√£o `model_list`.

## üìä Modelos Dispon√≠veis

| Modelo | Tamanho | Uso Recomendado | Fun√ß√£o Calling | Vis√£o |
|--------|---------|-----------------|----------------|-------|
| llama3.2 | 3B | Geral, r√°pido | ‚úÖ | ‚ùå |
| llama3.1 | 8B | Geral, poderoso | ‚úÖ | ‚ùå |
| mistral | 7B | Geral, eficiente | ‚ùå | ‚ùå |
| phi3 | 3.8B | Tarefas espec√≠ficas | ‚ùå | ‚ùå |
| gemma2 | 9B | Geral | ‚ùå | ‚ùå |
| codellama | 7B | Programa√ß√£o | ‚ùå | ‚ùå |
| llava | 7B | An√°lise de imagens | ‚ùå | ‚úÖ |

## üõ†Ô∏è Comandos √öteis

### Gerenciar Modelos Ollama

```bash
# Listar modelos instalados
docker exec -it ollama ollama list

# Baixar um modelo
docker exec -it ollama ollama pull <nome-do-modelo>

# Remover um modelo
docker exec -it ollama ollama rm <nome-do-modelo>

# Testar um modelo diretamente
docker exec -it ollama ollama run llama3.2 "Ol√°, como voc√™ est√°?"
```

### Gerenciar Containers

```bash
# Ver logs do LiteLLM
docker-compose logs -f litellm

# Ver logs do Ollama
docker-compose logs -f ollama

# Reiniciar servi√ßos
docker-compose restart

# Parar todos os servi√ßos
docker-compose down

# Parar e remover volumes (CUIDADO: apaga dados!)
docker-compose down -v
```

### Backup do Banco de Dados

```bash
# Fazer backup
docker exec ollama-litellm-postgres pg_dump -U litellm litellm > backup.sql

# Restaurar backup
cat backup.sql | docker exec -i ollama-litellm-postgres psql -U litellm litellm
```

## üîç Troubleshooting

### Problema: Ollama n√£o inicia

**Solu√ß√£o**: Verifique se a porta 11434 est√° dispon√≠vel:
```bash
sudo lsof -i :11434
docker-compose logs ollama
```

### Problema: LiteLLM n√£o conecta ao Ollama

**Solu√ß√£o**: Verifique se o Ollama est√° rodando:
```bash
curl http://localhost:11434/api/tags
```

### Problema: Erro de mem√≥ria ao rodar modelos

**Solu√ß√£o**: Use modelos menores ou aumente a RAM dispon√≠vel:
- Llama 3.2 (3B) requer ~4GB RAM
- Llama 3.1 (8B) requer ~8GB RAM
- Mistral (7B) requer ~6GB RAM

### Problema: PostgreSQL n√£o inicia

**Solu√ß√£o**: Verifique permiss√µes e logs:
```bash
docker-compose logs postgres
sudo chown -R 999:999 ./postgres_data  # Se necess√°rio
```

## üîó Integra√ß√£o com SetupOrion (Opcional)

Este stack tamb√©m est√° dispon√≠vel como **op√ß√£o [31]** no menu do SetupOrion para instala√ß√£o automatizada.

Se voc√™ estiver usando o SetupOrion:
1. Execute o SetupOrion
2. Escolha a op√ß√£o **[31]** - Ollama + LiteLLM
3. O instalador ser√° executado automaticamente

**Nota**: A instala√ß√£o via SetupOrion usa exatamente os mesmos arquivos e processo descrito neste README. O SetupOrion apenas facilita o acesso ao instalador.

## üìö Recursos Adicionais

- [Documenta√ß√£o Ollama](https://github.com/ollama/ollama)
- [Documenta√ß√£o LiteLLM](https://docs.litellm.ai/)
- [Modelos dispon√≠veis no Ollama](https://ollama.com/library)
- [SetupOrion](https://github.com/oriondesign2015/SetupOrion) - Instalador automatizado de aplica√ß√µes
- [API Reference LiteLLM](https://docs.litellm.ai/docs/proxy/endpoints)

## üîê Seguran√ßa

### Recomenda√ß√µes:

1. **Altere todas as senhas padr√£o** no arquivo `.env`
2. **Nunca exponha** a porta do PostgreSQL (5432) publicamente
3. **Use HTTPS** em produ√ß√£o (configure um reverse proxy como Traefik/Nginx)
4. **Limite o acesso** √† interface web do LiteLLM
5. **Fa√ßa backups regulares** do banco de dados
6. **Monitore o uso** atrav√©s da interface web

## ü§ù Suporte

Para problemas ou d√∫vidas:
- Abra uma issue no reposit√≥rio
- Entre em contato com a comunidade OrionDesign
- Consulte a documenta√ß√£o oficial dos projetos

## üìÑ Licen√ßa

Este stack segue a licen√ßa MIT do projeto SetupOrion.

---

**Desenvolvido por OrionDesign** | [oriondesign.art.br](https://oriondesign.art.br)
**Desenvolvido por BEMN** | [bemn.com.br](https://bemn.com.br)
