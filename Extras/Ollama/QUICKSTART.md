# üöÄ Guia de In√≠cio R√°pido - Ollama + LiteLLM

**‚ú® Instala√ß√£o Standalone**: Este stack √© completamente independente e pode ser instalado sem o SetupOrion.

## Instala√ß√£o em 3 Passos

### 1Ô∏è‚É£ Baixe e execute o instalador

```bash
# Clone o reposit√≥rio (ou baixe apenas a pasta Ollama)
git clone https://github.com/oriondesign2015/SetupOrion.git
cd SetupOrion/Extras/Ollama

# Execute o instalador
bash install.sh
```

O instalador ir√°:
- ‚úÖ Verificar depend√™ncias (Docker, Docker Compose)
- ‚úÖ Criar diret√≥rio de instala√ß√£o em `~/ollama-litellm`
- ‚úÖ Copiar todos os arquivos necess√°rios
- ‚úÖ Detectar PostgreSQL existente no sistema
- ‚úÖ Configurar vari√°veis de ambiente com chaves seguras
- ‚úÖ Iniciar os servi√ßos
- ‚úÖ Opcionalmente baixar um modelo

### 2Ô∏è‚É£ Aguarde os servi√ßos iniciarem

```bash
# V√° para o diret√≥rio de instala√ß√£o
cd ~/ollama-litellm

# Verificar status
docker-compose ps

# Ver logs
docker-compose logs -f
```

### 3Ô∏è‚É£ Teste a instala√ß√£o

```bash
# Testar Ollama
curl http://localhost:11434/api/tags

# Testar LiteLLM
curl http://localhost:4000/health
```

## üéØ Primeiro Uso

### Baixar um modelo (se n√£o baixou durante instala√ß√£o)

```bash
# Llama 3.2 (3B) - R√°pido e eficiente
docker exec ollama ollama pull llama3.2

# Ou Mistral (7B) - √ìtimo equil√≠brio
docker exec ollama ollama pull mistral
```

### Fazer sua primeira requisi√ß√£o

```bash
curl -X POST 'http://localhost:4000/chat/completions' \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer SEU_MASTER_KEY_AQUI' \
  -d '{
    "model": "llama3.2",
    "messages": [
      {
        "role": "user",
        "content": "Ol√°! Me explique o que √© intelig√™ncia artificial em 3 linhas."
      }
    ]
  }'
```

**Nota**: Substitua `SEU_MASTER_KEY_AQUI` pela chave que foi gerada (est√° no arquivo `.env` como `LITELLM_MASTER_KEY`)

### Acessar a interface web

1. Abra seu navegador em: `http://localhost:4000/ui`
2. Fa√ßa login com sua `LITELLM_MASTER_KEY`
3. Explore a interface de administra√ß√£o

## üêç Exemplo com Python

```python
from openai import OpenAI

# Configurar cliente
client = OpenAI(
    api_key="SEU_MASTER_KEY_AQUI",  # Sua LITELLM_MASTER_KEY
    base_url="http://localhost:4000"
)

# Fazer uma requisi√ß√£o
response = client.chat.completions.create(
    model="llama3.2",
    messages=[
        {"role": "system", "content": "Voc√™ √© um assistente √∫til."},
        {"role": "user", "content": "Qual √© a capital do Brasil?"}
    ]
)

print(response.choices[0].message.content)
```

## üì¶ Instala√ß√£o do SDK Python

```bash
pip install openai
```

## üîë Criar Chaves Virtuais

Chaves virtuais permitem controlar acesso, limites e rastreamento:

```bash
curl -X POST 'http://localhost:4000/key/generate' \
  -H 'Authorization: Bearer SEU_MASTER_KEY_AQUI' \
  -H 'Content-Type: application/json' \
  -d '{
    "key_alias": "minha-aplicacao",
    "rpm_limit": 10,
    "max_budget": 100,
    "models": ["llama3.2", "mistral"]
  }'
```

Isso retornar√° uma nova chave que voc√™ pode usar em suas aplica√ß√µes.

## üõ†Ô∏è Comandos √öteis

```bash
# Ver logs em tempo real
docker-compose logs -f

# Reiniciar servi√ßos
docker-compose restart

# Parar servi√ßos
docker-compose down

# Parar e remover volumes (CUIDADO: apaga dados!)
docker-compose down -v

# Listar modelos instalados
docker exec ollama ollama list

# Remover um modelo
docker exec ollama ollama rm llama3.2

# Ver uso de recursos
docker stats
```

## üîß Configura√ß√µes Avan√ßadas

### Alterar portas

Edite o arquivo `.env`:

```bash
OLLAMA_PORT=11434
LITELLM_PORT=4000
```

Depois reinicie:

```bash
docker-compose down
docker-compose up -d
```

### Adicionar mais modelos no LiteLLM

Edite `litellm_config.yaml` e adicione:

```yaml
model_list:
  - model_name: seu-modelo
    litellm_params:
      model: ollama_chat/seu-modelo
      api_base: http://ollama:11434
```

Reinicie o LiteLLM:

```bash
docker-compose restart litellm
```

## ‚ùì Problemas Comuns

### Erro: "Connection refused"

**Solu√ß√£o**: Aguarde os servi√ßos iniciarem completamente (pode levar 1-2 minutos)

```bash
docker-compose logs -f
```

### Erro: "Model not found"

**Solu√ß√£o**: Baixe o modelo primeiro

```bash
docker exec ollama ollama pull llama3.2
```

### Erro: "Out of memory"

**Solu√ß√£o**: Use um modelo menor ou aumente a RAM dispon√≠vel

- Llama 3.2 (3B) ‚Üí ~4GB RAM
- Phi-3 (3.8B) ‚Üí ~4GB RAM
- Mistral (7B) ‚Üí ~6GB RAM

## üìö Pr√≥ximos Passos

1. ‚úÖ Leia a documenta√ß√£o completa: `README.md`
2. ‚úÖ Explore a interface web em `http://localhost:4000/ui`
3. ‚úÖ Teste diferentes modelos
4. ‚úÖ Configure chaves virtuais para suas aplica√ß√µes
5. ‚úÖ Integre com suas aplica√ß√µes usando o SDK OpenAI

## üÜò Suporte

- üìñ Documenta√ß√£o completa: `README.md`
- üåê Ollama: https://github.com/ollama/ollama
- üåê LiteLLM: https://docs.litellm.ai/
- üí¨ Comunidade OrionDesign

---

**Desenvolvido por OrionDesign** | [oriondesign.art.br](https://oriondesign.art.br)

